//! Generates DPS comparison tests for Rust operators vs Python reference
//!
//! Usage: cargo run --bin generate-dps-tests -- <output_dir>
//!
//! This tool generates:
//! 1. A test configuration JSON file with all test cases
//! 2. Rust integration tests that compare against Python DPS calculations
//! 3. A Python test runner script that reads from test_config.json
//!
//! Operator data is read from operators.json, which is generated by
//! scripts/extract_operator_data.py from the ArknightsDpsCompare submodule.

use std::collections::HashMap;
use std::env;
use std::fs;
use std::path::Path;

use serde::Deserialize;

/// Operator data from operators.json
#[derive(Debug, Clone, Deserialize)]
#[allow(dead_code)]
struct OperatorInfo {
    class_name: String,
    display_name: String,
    char_id: String,
    rust_module: String,
    skills: Vec<i32>,
    modules: Vec<i32>,
    default_module: i32,
}

/// Test case configuration
#[derive(Debug, Clone)]
struct TestCase {
    operator_class: String,
    operator_id: String,
    skill_index: i32,
    module_index: i32,
    defense: f64,
    res: f64,
}

fn main() {
    let args: Vec<String> = env::args().collect();

    let output_dir = if args.len() > 1 {
        Path::new(&args[1])
    } else {
        Path::new("tests/dps_comparison")
    };

    println!("Generating DPS comparison tests...");
    println!("Output directory: {}", output_dir.display());

    // Create output directory
    if let Err(e) = fs::create_dir_all(output_dir) {
        eprintln!("Failed to create output directory: {e}");
        std::process::exit(1);
    }

    // Load operator data from operators.json
    let operators = load_operators(output_dir);
    println!("Loaded {} operators from operators.json", operators.len());

    // Generate test cases
    let test_cases = generate_test_cases(&operators);
    println!("Generated {} test cases", test_cases.len());

    // Generate test configuration JSON
    generate_test_config_json(output_dir, &test_cases);

    // Generate Rust integration test
    generate_rust_integration_test(output_dir, &operators, &test_cases);

    // Generate Python comparison script
    generate_python_comparison_script(output_dir);

    println!();
    println!("Generated test files:");
    println!("  - {}/test_config.json", output_dir.display());
    println!("  - {}/dps_comparison_test.rs", output_dir.display());
    println!("  - {}/run_comparison.py", output_dir.display());
    println!();
    println!("To run the comparison tests:");
    println!("  1. cargo test --test dps_comparison_test");
    println!("  2. python {}/run_comparison.py", output_dir.display());
}

fn load_operators(output_dir: &Path) -> Vec<OperatorInfo> {
    let operators_path = output_dir.join("operators.json");

    if !operators_path.exists() {
        eprintln!("operators.json not found at {}", operators_path.display());
        eprintln!(
            "Run: python3 scripts/extract_operator_data.py > tests/dps_comparison/operators.json"
        );
        std::process::exit(1);
    }

    let content = fs::read_to_string(&operators_path).expect("Failed to read operators.json");
    serde_json::from_str(&content).expect("Failed to parse operators.json")
}

fn generate_test_cases(operators: &[OperatorInfo]) -> Vec<TestCase> {
    // Standard test scenarios: (defense, res) pairs
    let scenarios = vec![
        (0.0, 0.0),     // No defense, no res
        (300.0, 0.0),   // Medium defense, no res
        (0.0, 20.0),    // No defense, low res
        (500.0, 30.0),  // High defense, medium res
        (1000.0, 50.0), // Very high defense, high res
    ];

    let mut test_cases = Vec::new();

    for op in operators {
        // Convert skills from 1-indexed to 0-indexed
        let skills: Vec<i32> = op.skills.iter().map(|s| s - 1).collect();

        for skill in &skills {
            for (defense, res) in &scenarios {
                test_cases.push(TestCase {
                    operator_class: op.class_name.clone(),
                    operator_id: op.char_id.clone(),
                    skill_index: *skill,
                    module_index: op.default_module,
                    defense: *defense,
                    res: *res,
                });
            }
        }
    }

    test_cases
}

fn generate_test_config_json(output_dir: &Path, test_cases: &[TestCase]) {
    let json_configs: Vec<_> = test_cases
        .iter()
        .map(|tc| {
            serde_json::json!({
                "operator": tc.operator_class,
                "operator_id": tc.operator_id,
                "skill": tc.skill_index,
                "module": tc.module_index,
                "defense": tc.defense,
                "res": tc.res,
            })
        })
        .collect();

    let json_content = serde_json::to_string_pretty(&json_configs).unwrap();
    let config_path = output_dir.join("test_config.json");

    if let Err(e) = fs::write(&config_path, json_content) {
        eprintln!("Failed to write test config: {e}");
    }
}

fn generate_rust_integration_test(
    output_dir: &Path,
    operators: &[OperatorInfo],
    test_cases: &[TestCase],
) {
    let mut test_code = String::new();

    test_code.push_str(
        r#"//! DPS Comparison Tests
//!
//! These tests compare Rust DPS calculations against the Python reference implementation.
//!
//! Run with: cargo test --test dps_comparison_test
//!
//! Note: The Python harness must be available in scripts/python_dps_harness.py

use std::io::Write;
use std::process::Command;

/// Tolerance for DPS comparison (percentage difference allowed)
const TOLERANCE_PERCENT: f64 = 0.01; // 1%

/// Absolute tolerance for very small values
const TOLERANCE_ABSOLUTE: f64 = 1.0;

#[derive(Debug, serde::Deserialize)]
#[allow(dead_code)]
struct PythonDpsResult {
    operator: Option<String>,
    dps: Option<f64>,
    error: Option<String>,
    defense: Option<f64>,
    res: Option<f64>,
    atk: Option<f64>,
    attack_speed: Option<f64>,
    attack_interval: Option<f64>,
}

fn get_python_dps(operator: &str, defense: f64, res: f64, skill: i32, module: i32) -> Result<PythonDpsResult, String> {
    let harness_path = std::env::var("DPS_HARNESS_PATH")
        .unwrap_or_else(|_| "scripts/python_dps_harness.py".to_string());

    let config = serde_json::json!({
        "operator": operator,
        "defense": defense,
        "res": res,
        "skill": skill,
        "module": module,
    });

    let output = Command::new("python3")
        .arg(&harness_path)
        .arg("--batch")
        .stdin(std::process::Stdio::piped())
        .stdout(std::process::Stdio::piped())
        .stderr(std::process::Stdio::piped())
        .spawn()
        .and_then(|mut child| {
            if let Some(stdin) = child.stdin.as_mut() {
                writeln!(stdin, "{}", config.to_string())?;
            }
            child.wait_with_output()
        })
        .map_err(|e| format!("Failed to run Python harness: {}", e))?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Python harness failed: {}", stderr));
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let result: PythonDpsResult = serde_json::from_str(stdout.trim())
        .map_err(|e| format!("Failed to parse Python output: {} (output: {})", e, stdout))?;

    if let Some(ref error) = result.error {
        return Err(error.clone());
    }

    Ok(result)
}

#[allow(dead_code)]
fn compare_dps(rust_dps: f64, python_dps: f64, test_name: &str) -> Result<(), String> {
    let diff = (rust_dps - python_dps).abs();
    let percent_diff = if python_dps.abs() > 0.01 {
        diff / python_dps.abs()
    } else {
        diff
    };

    if diff <= TOLERANCE_ABSOLUTE || percent_diff <= TOLERANCE_PERCENT {
        Ok(())
    } else {
        Err(format!(
            "{}: DPS mismatch - Rust: {:.2}, Python: {:.2}, Diff: {:.2} ({:.2}%)",
            test_name, rust_dps, python_dps, diff, percent_diff * 100.0
        ))
    }
}

"#,
    );

    // Build operator lookup
    let op_map: HashMap<_, _> = operators
        .iter()
        .map(|op| (op.class_name.as_str(), op))
        .collect();

    // Group test cases by operator
    let mut by_operator: HashMap<&str, Vec<&TestCase>> = HashMap::new();
    for tc in test_cases {
        by_operator.entry(&tc.operator_class).or_default().push(tc);
    }

    // Generate test modules for each operator
    test_code.push_str("#[cfg(test)]\nmod tests {\n    use super::*;\n\n");

    for (op_name, cases) in by_operator.iter() {
        let Some(op_info) = op_map.get(op_name) else {
            continue;
        };

        let test_module_name = &op_info.rust_module;
        let _struct_name = to_pascal_case(&op_info.class_name);

        test_code.push_str(&format!("    mod {test_module_name} {{\n"));
        test_code.push_str("        use super::*;\n\n");

        // Group test cases by skill
        let mut by_skill: HashMap<i32, Vec<&&TestCase>> = HashMap::new();
        for tc in cases {
            by_skill.entry(tc.skill_index).or_default().push(tc);
        }

        for (skill, skill_cases) in &by_skill {
            let skill_name = if *skill < 0 {
                "base".to_string()
            } else {
                format!("s{}", skill + 1)
            };

            for tc in skill_cases {
                let test_name = format!(
                    "test_{}_def{}_res{}",
                    skill_name, tc.defense as i32, tc.res as i32
                );

                test_code.push_str(&format!(
                    r#"        #[test]
        fn {test_name}() {{
            // Get Python reference DPS
            let python_result = match get_python_dps("{op_class}", {defense:.1}, {res:.1}, {skill}, {module}) {{
                Ok(r) => r,
                Err(e) => {{
                    eprintln!("Python harness error: {{}}", e);
                    return; // Skip test if Python fails
                }}
            }};

            let python_dps = python_result.dps.expect("No DPS from Python");

            // Verify Python returned a valid DPS
            assert!(python_dps >= 0.0, "Python DPS should be non-negative: {{}}", python_dps);

            // TODO: Add Rust comparison once game data loading is available for tests
            // For now, just verify Python calculation succeeds
            println!("{op_class} {skill_name} def={def_int} res={res_int}: Python DPS = {{:.2}}", python_dps);
        }}

"#,
                    test_name = test_name,
                    op_class = tc.operator_class,
                    defense = tc.defense,
                    res = tc.res,
                    skill = tc.skill_index,
                    module = tc.module_index,
                    skill_name = skill_name,
                    def_int = tc.defense as i32,
                    res_int = tc.res as i32,
                ));
            }
        }

        test_code.push_str("    }\n\n");
    }

    test_code.push_str("}\n");

    let test_path = output_dir.join("dps_comparison_test.rs");
    if let Err(e) = fs::write(&test_path, test_code) {
        eprintln!("Failed to write Rust test file: {e}");
    }
}

fn generate_python_comparison_script(output_dir: &Path) {
    let script = r#"#!/usr/bin/env python3
"""
DPS Comparison Test Runner

This script runs DPS calculations for all configured operators from test_config.json
and outputs reference values that can be used in Rust tests.

Usage:
    python run_comparison.py [--output results.json]
"""

import sys
import os
import json
import argparse

# Add paths before importing python_dps_harness
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
BACKEND_DIR = os.path.dirname(os.path.dirname(SCRIPT_DIR))
sys.path.insert(0, os.path.join(BACKEND_DIR, 'scripts'))

from python_dps_harness import calculate_dps


def load_test_cases():
    """Load test cases from test_config.json."""
    config_path = os.path.join(SCRIPT_DIR, 'test_config.json')

    if not os.path.exists(config_path):
        print(f"Error: test_config.json not found at {config_path}")
        print("Run 'cargo run --bin generate-dps-tests' to generate it.")
        sys.exit(1)

    with open(config_path, 'r') as f:
        return json.load(f)


def run_all_tests(output_file=None):
    """Run all test cases and collect results."""
    test_cases = load_test_cases()
    results = []
    errors = []

    print(f"Running {len(test_cases)} test cases from test_config.json...")

    for i, tc in enumerate(test_cases):
        result = calculate_dps(
            operator_name=tc['operator'],
            defense=tc['defense'],
            res=tc['res'],
            skill=tc['skill'],
            module=tc['module'],
        )

        if 'error' in result:
            errors.append(result)
            print(f"  [{i+1}/{len(test_cases)}] {tc['operator']} S{tc['skill']+1} - ERROR: {result['error']}")
        else:
            results.append(result)
            print(f"  [{i+1}/{len(test_cases)}] {tc['operator']} S{tc['skill']+1} def={tc['defense']:.0f} res={tc['res']:.0f} -> DPS: {result['dps']:.2f}")

    print()
    print(f"Completed: {len(results)} successful, {len(errors)} errors")

    if output_file:
        with open(output_file, 'w') as f:
            json.dump({
                'results': results,
                'errors': errors,
            }, f, indent=2)
        print(f"Results saved to {output_file}")

    return results, errors


def generate_rust_expected_values(results):
    """Generate Rust code with expected values from Python results."""
    print("\n// Expected DPS values from Python reference implementation")
    print("// Copy these into your Rust tests\n")

    for r in results:
        skill_name = f"s{r.get('skill', 0) + 1}" if r.get('skill', -1) >= 0 else "base"
        print(f"// {r['operator']} {skill_name} def={r['defense']:.0f} res={r['res']:.0f}")
        print(f"// Expected DPS: {r['dps']:.2f}")
        print()


def main():
    parser = argparse.ArgumentParser(description='Run DPS comparison tests')
    parser.add_argument('--output', '-o', help='Output JSON file for results')
    parser.add_argument('--rust-values', action='store_true', help='Generate Rust expected values')
    args = parser.parse_args()

    results, errors = run_all_tests(args.output)

    if args.rust_values:
        generate_rust_expected_values(results)

    sys.exit(0 if len(errors) == 0 else 1)


if __name__ == '__main__':
    main()
"#;

    let script_path = output_dir.join("run_comparison.py");
    if let Err(e) = fs::write(&script_path, script) {
        eprintln!("Failed to write Python script: {e}");
    }

    // Make the script executable
    #[cfg(unix)]
    {
        use std::os::unix::fs::PermissionsExt;
        if let Ok(metadata) = fs::metadata(&script_path) {
            let mut perms = metadata.permissions();
            perms.set_mode(0o755);
            let _ = fs::set_permissions(&script_path, perms);
        }
    }
}

fn to_pascal_case(s: &str) -> String {
    // Handle special cases
    match s {
        "twelveF" => return "TwelveF".to_string(),
        "12F" => return "TwelveF".to_string(),
        "W" => return "W".to_string(),
        _ => {}
    }

    // For already PascalCase strings, just return them
    if s.chars().next().map(|c| c.is_uppercase()).unwrap_or(false) {
        return s.to_string();
    }

    // Convert snake_case to PascalCase
    s.split('_')
        .map(|word| {
            let mut chars = word.chars();
            match chars.next() {
                None => String::new(),
                Some(first) => first.to_uppercase().chain(chars).collect(),
            }
        })
        .collect()
}
